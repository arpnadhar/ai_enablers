cmd
cd C:\Users\52243042\Downloads\B-Druck\Portrait-AI\MODNet-Single-Pic
C:\Users\52243042\Downloads\B-Druck\Portrait-AI\MODNet-Single-Pic>del IMAGES\input\*_matte* IMAGES\input\*fg*
C:\Users\52243042\Downloads\B-Druck\Portrait-AI\MODNet-Single-Pic>python runCommandEngine.py
==================
#Working Single
C:\Users\52243042\Downloads\B-Druck\Portrait-AI\MODNet-Single-Pic>python demo/image_matting/Inference_with_ONNX/inference_onnx.py --image-path=IMAGES/input/"africanboys group.jpg" --output-path=IMAGES/output/matte.png --model-path=pretrained/modnet.onnx
Working Single
C:\Users\52243042\Downloads\B-Druck\Portrait-AI\MODNet-Single-Pic>python demo/image_matting/Inference_with_ONNX/inference_onnx.py --image-path=IMAGES/input/"africanboys group.jpg" --output-path=IMAGES/output/matte.png --model-path=pretrained/modnet.onnx
==================

Working Multiple
==================
$ pwd
/c/Users/52243042/Downloads/B-Druck/MODNet

52243042@NODLTP52243042 MINGW64 ~/Downloads/B-Druck/MODNet (master)
$ python -m demo.image_matting.colab.inference --input-path demo/image_matting/colab/input --output-path demo/image_matting/colab/output --ckpt-path ./pretrained/modnet_photographic_portrait_matting.ckpt

C:\Users\52243042\AppData\Roaming\Python\Python39\site-packages\torch\nn\functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.
  warnings.warn(
Process image: african boys.jpg
Process image: Brie-Grilled-Cheese-Fifteen-Spatulas-2-640x427.jpg
Process image: Lal-Maas.jpeg
Process image: Sushil-FB-2.jpg
Process image: _After_Processing.JPG
==================

52243042@NODLTP52243042 MINGW64 ~/Downloads/B-Druck
$ which jupyter
/c/Users/52243042/AppData/Roaming/Python/Python39/Scripts/jupyter

52243042@NODLTP52243042 MINGW64 ~/Downloads/B-Druck
$ jupyter notebook


pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/index.html

pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html

pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.8/index.html


source detectron2/Scripts/activate

pip install -r detectron2/requirements.txt

python.exe -m pip install --upgrade pip

pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/index.html

pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html

python detectron2/demo.py --config-file detectron2/lib/python3.7/site-packages/detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --input input/* --output output  --opts MODEL.DEVICE cpu MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl


python detectron2/demo.py --config-file detectron2/lib/python3.7/site-packages/detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --input input/* --output output  --opts MODEL.DEVICE cpu MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl



git clone https://github.com/facebookresearch/detectron2


Run frame-by-frame inference demo on this video (takes 3-4 minutes) with the "demo.py" tool we provided in the repo.
!git clone https://github.com/facebookresearch/detectron2
# Note: this is currently BROKEN due to missing codec. See https://github.com/facebookresearch/detectron2/issues/2901 for workaround.
%run detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output video-output.mkv \
  --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl

C:\Users\52243042\AppData\Roaming\Python\Python39\Scripts

pip install imgaug
pip install Cython
pip install pycocotools
pip install kaggle
pip install pyyaml==5.1
pip install youtube-dl
pip3 show torch



python detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output video-output.mkv \
  --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl



python -m demo.image_matting.colab.inference --input-path demo/image_matting/colab/input --output-path demo/image_matting/colab/output --ckpt-path ./pretrained/modnet_photographic_portrait_matting.ckpt


python -m demo/image_matting/colab/inference --input-path demo/image_matting/colab/input --output-path demo/image_matting/colab/output --ckpt-path ./pretrained/modnet_photographic_portrait_matting.ckpt


pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio===0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html

pip3 install torch==10.2 torchvision==10.2 torchaudio===10.2 -f https://download.pytorch.org/whl/cu113/torch_stable.html

pip install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch

pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html

pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html

pip3 install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html

pip3 show torch


pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html

pip install torch==1.10.0+cu102-cp39-cp39 torchvision==0.10.1+cu102-cp39-cp39 torchaudio===0.10.1+cu102-cp39-cp39 -f https://download.pytorch.org/whl/torch_stable.html

https://download.pytorch.org/whl/torch_stable.html


def combined_display(image, matte):
  # calculate display resolution
  w, h = image.width, image.height
  rw, rh = 800, int(h * 800 / (3 * w))
  
  # obtain predicted foreground
  image = np.asarray(image)
  if len(image.shape) == 2:
    image = image[:, :, None]
  if image.shape[2] == 1:
    image = np.repeat(image, 3, axis=2)
  elif image.shape[2] == 4:
    image = image[:, :, 0:3]
  matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2) / 255
  foreground = image * matte + np.full(image.shape, 255) * (1 - matte)


  #Added by Sushil
        #obtain predicted foreground
	#  im = np.asarray(im)
	  if len(im.shape) == 2:
	    im = im[:, :, None]
	  if im.shape[2] == 1:
	    im = np.repeat(im, 3, axis=2)
	  elif im.shape[2] == 4:
	    im = im[:, :, 0:3]
	  matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2) / 255
 	 foreground = im * matte + np.full(im.shape, 255) * (1 - matte)
        #Added by Sushil





52243042@NODLTP52243042 MINGW64 ~/Downloads/B-Druck/MODNet (master)

pip install onnx onnxruntime

 python -m onnx.export_onnx --ckpt-path=pretrained/modnet_photographic_portrait_matting.ckpt --output-path=pretrained/modnet_photographic_portrait_matting.onnx



python demo/image_matting/Inference_with_ONNX/inference_onnx.py --image-path=IMAGES/input/image.jpg --output-path=IMAGES/output/matte.png --model-path=pretrained/modnet.onnx

python demo/image_matting/Inference_with_ONNX/inference_onnx.py --image-path=IMAGES/input/image.jpg --output-path=IMAGES/output/matte.png --model-path=pretrained/modnet_photographic_portrait_matting.ckpt

python demo/image_matting/Inference_with_ONNX/export_modnet_onnx.py --ckpt-path=pretrained/modnet_photographic_portrait_matting.ckpt --output-path=pretrained/modnet.onnx